This project uses a combined dataset of Rotten Tomatoes critic reviews and TMDb movie metadata to explore how review sentiment, budget, and engineered features relate to movie success, measured in both revenue and average rating. I started by cleaning over 100,000 reviews, standardizing reviewer scores across mixed grading formats, and engineering categorical features like genre and release month. I then applied both a lexicon-based sentiment analyzer (VADER) and a transformer model (DistilBERT via Flair) to the review text, comparing speed, interpretability, and contextual accuracy. VADER’s compound score tracked numeric reviewer ratings more closely, so it was used as a feature in the final models. I trained linear regression and classification models to predict revenue and rating tiers, finding that budget, vote count, and sentiment were consistent drivers of both outcomes. The results showed that even simple models can pick up meaningful patterns when the data is cleaned well, and that fast, interpretable tools like VADER still have value in applied NLP pipelines. I’m continuing to expand this project with additional modeling pipelines, GPU-based inference, and exploratory use cases in NLP, which is why the repo is currently being refactored and reorganized. Unlike my earlier undergrad projects, which relied on internal datasets and remain confidential, I’m able to share this code publicly thanks to the availability of the movie review data. This project summary is a work in progress.